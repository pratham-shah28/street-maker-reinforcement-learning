
# StreetStarter.ai
AI Urban Development Application for CS 5100 - Foundations of AI

In this project, we aim to develop a program that is capable of generating city development plans based on user-specified dimensions. To generate the map, the user will input their desired dimensions and other parameters. Since we are not extracting data from a source to generate our initial map, the algorithm will initialize a map within these specifications. To ensure user-friendly interaction and visualization, we will construct a two-dimensional graphical user interface (GUI) using the PyGame library to showcase the generated plans. Our primary focus lies in optimizing street and building size allocation within cities of varying dimensions through the use of reinforcement learning.  With the provided map dimensions, we aim to maximize city_score which will be a combination of different scores such as area usage, connectivity, etc. Each city will include constraints such as connectivity, building types, building size, population density, etc. We will have multiple scenarios that will result in a punishment to the final score. For example, any of the cells surrounding a road that is not a building will include a punishment to the final reward score for the generated map. 

## Starter usage

- Install dependencies
- Run main.py
- press 'h' to select house, 't' for townhall, 'r' to rotate building and click on the grid to place it.
- Feel free to add more buildings types. 

## Project Overview

StreetStarter.ai focuses on urban planning by generating city development plans. It uses Q-learning to help an agent discover optimal paths for connecting city buildings. The GUI visualizes these plans, offering an interactive way to explore city layouts.

This project explores reinforcement learning for city planning applications, optimizing both street and building layouts.

## Features

- **Reinforcement Learning (Q-learning):** Uses Q-learning to find optimal paths between buildings.
- **PyGame GUI:** Visualizes city layouts and paths generated by the Q-learning algorithm.
- **Dynamic Rewards System:** Penalizes unnecessary steps and rewards proximity to buildings.
- **Path Visualization:** Displays the shortest or optimal path in blue squares on the grid.

## Model and Methods

- **Q-learning Algorithm:** A model-free, reinforcement learning approach where an agent learns to connect buildings by receiving rewards and penalties.
- **Markov Decision Process (MDP):** Utilizes 225 states in a 15x15 grid, where the agent learns to move between buildings while avoiding obstacles.
- **Rewards Matrix:** Rewards of 2000 for reaching adjacent buildings and penalties for unnecessary steps or building collisions.

## Experimental Setup

- **Grid Environment:** 15x15 grid visualized via PyGame.
- **Agent's Actions:** Move up, down, left, or right, avoiding obstacles (buildings).
- **Hyperparameters:**
  - Learning rate (η): 0.97
  - Discount factor (γ): 0.97
  - Epsilon (ε): 0.97, decreases by a factor of 0.9999 per iteration to balance exploration and exploitation.

## Results

- The agent successfully connects different buildings in the grid.
- Increasing the number of iterations improves path efficiency, but with diminishing returns.
- Computational time increases significantly as the number of iterations grows, which can be mitigated by using Deep Q-learning in future work.
